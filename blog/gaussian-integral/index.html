<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>The Gaussian Integral | Hanbin Zheng</title><meta name=keywords content="Math,Probability,PCA,Gaussian"><meta name=description content="
This post introduces the Gaussian integral, starting from basic integration and gradually building towards a probabilistic and linear algebraic perspective.

The exposition proceeds from a purely mathematical derivation to a more intuitive understanding grounded in probability and linear algebra.
I. The Fundamental Theorem
The most fundamental Gaussian Integral is as follows:
$$
\int_{-\infty}^{\infty} \exp \left(-\frac{1}{2}x^2 \right) dx = \sqrt{2\pi}
$$
Equivalently, in the form of the standard Gaussian distribution:"><meta name=author content><link rel=canonical href=https://hanbinzheng.github.io/blog/gaussian-integral/><link crossorigin=anonymous href=/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css integrity="sha256-j+ECM6cGvIfy4Is8+XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as=style><link rel=icon href=https://hanbinzheng.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://hanbinzheng.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://hanbinzheng.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://hanbinzheng.github.io/apple-touch-icon.png><link rel=mask-icon href=https://hanbinzheng.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://hanbinzheng.github.io/blog/gaussian-integral/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],packages:{"[+]":["ams"]}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js defer></script><meta property="og:url" content="https://hanbinzheng.github.io/blog/gaussian-integral/"><meta property="og:site_name" content="Hanbin Zheng"><meta property="og:title" content="The Gaussian Integral"><meta property="og:description" content=" This post introduces the Gaussian integral, starting from basic integration and gradually building towards a probabilistic and linear algebraic perspective.
The exposition proceeds from a purely mathematical derivation to a more intuitive understanding grounded in probability and linear algebra.
I. The Fundamental Theorem The most fundamental Gaussian Integral is as follows:
$$ \int_{-\infty}^{\infty} \exp \left(-\frac{1}{2}x^2 \right) dx = \sqrt{2\pi} $$
Equivalently, in the form of the standard Gaussian distribution:"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-05-19T00:00:00+08:00"><meta property="article:modified_time" content="2025-05-19T00:00:00+08:00"><meta property="article:tag" content="Math"><meta property="article:tag" content="Probability"><meta property="article:tag" content="PCA"><meta property="article:tag" content="Gaussian"><meta name=twitter:card content="summary"><meta name=twitter:title content="The Gaussian Integral"><meta name=twitter:description content="
This post introduces the Gaussian integral, starting from basic integration and gradually building towards a probabilistic and linear algebraic perspective.

The exposition proceeds from a purely mathematical derivation to a more intuitive understanding grounded in probability and linear algebra.
I. The Fundamental Theorem
The most fundamental Gaussian Integral is as follows:
$$
\int_{-\infty}^{\infty} \exp \left(-\frac{1}{2}x^2 \right) dx = \sqrt{2\pi}
$$
Equivalently, in the form of the standard Gaussian distribution:"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://hanbinzheng.github.io/blog/"},{"@type":"ListItem","position":2,"name":"The Gaussian Integral","item":"https://hanbinzheng.github.io/blog/gaussian-integral/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"The Gaussian Integral","name":"The Gaussian Integral","description":" This post introduces the Gaussian integral, starting from basic integration and gradually building towards a probabilistic and linear algebraic perspective.\nThe exposition proceeds from a purely mathematical derivation to a more intuitive understanding grounded in probability and linear algebra.\nI. The Fundamental Theorem The most fundamental Gaussian Integral is as follows:\n$$ \\int_{-\\infty}^{\\infty} \\exp \\left(-\\frac{1}{2}x^2 \\right) dx = \\sqrt{2\\pi} $$\nEquivalently, in the form of the standard Gaussian distribution:\n","keywords":["Math","Probability","PCA","Gaussian"],"articleBody":" This post introduces the Gaussian integral, starting from basic integration and gradually building towards a probabilistic and linear algebraic perspective.\nThe exposition proceeds from a purely mathematical derivation to a more intuitive understanding grounded in probability and linear algebra.\nI. The Fundamental Theorem The most fundamental Gaussian Integral is as follows:\n$$ \\int_{-\\infty}^{\\infty} \\exp \\left(-\\frac{1}{2}x^2 \\right) dx = \\sqrt{2\\pi} $$\nEquivalently, in the form of the standard Gaussian distribution:\n$$ \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\exp \\left( -\\frac{1}{2} x^2 \\right) dx = 1 $$\nHere is how to compute this integral. For convenience, denote this integral by $\\mathrm{I}$.\nStep1 $$ \\begin{align*} \\mathrm{I} \u0026= \\int_{-\\infty}^{\\infty} \\exp \\left( -\\frac{1}{2}x^2 \\right) dx \\ \\ \\mathrm{I}^2 \u0026= \\int_{-\\infty}^{\\infty} \\exp \\left( -\\frac{1}{2} x^2 \\right) dx ; \\cdot ; \\int_{-\\infty}^{\\infty} \\exp \\left( -\\frac{1}{2} y^2 \\right) dy \\ \\ \u0026= \\iint_{\\mathbb{R}^2} \\exp \\left( -\\frac{1}{2}(x^2 + y^2) \\right) dx ; dy \\end{align*} $$\nStep2 Transform from Cartesian Coordinates to Polar Coordinates, where\n$$ \\left { \\begin{align*} x \u0026= r ; \\cos (\\theta) \\ y \u0026= r ; \\sin (\\theta) \\end{align*} \\right. $$\nAnd the Jacobian matrix for this transform is:\n$$ \\mathrm{J}(r, \\theta) = \\frac{\\partial (x, y)}{\\partial (r, \\theta)} =\\begin{bmatrix} \\frac{\\partial x}{\\partial r} \u0026 \\frac{\\partial x}{\\partial \\theta} \\ \\frac{\\partial y}{\\partial r} \u0026 \\frac{\\partial y}{\\partial \\theta} \\end{bmatrix} = \\begin{bmatrix} \\cos (\\theta) \u0026 - r \\sin (\\theta) \\ \\sin (\\theta) \u0026 r \\cos (\\theta) \\end{bmatrix} $$\nand the transform from $dx ; dy$$ to $$dr ; d \\theta$ is:\n$$ \\begin{align*} dx ; dy \u0026= \\det \\left( \\mathrm{J}(r, \\theta) \\right) dr ; d \\theta \\ \\ \u0026= r \\left( \\cos^2 (\\theta) + \\sin^2 (\\theta) \\right) ;dr ; d \\theta\\ \\ \u0026= r ; dr ; d \\theta \\end{align*} $$\nStep3 So after the transform, the equation for integral $\\mathrm{I}$ is:\n$$ \\begin{align*} \\mathrm{I}^2 \u0026= \\iint_{\\mathbb{R}^2}\\exp \\left( -\\frac{1}{2} r^2 \\right) ; r ; dr ; d \\theta \\ \\ \u0026= \\int_0^{2 \\pi} \\left( \\int_0^{\\infty} \\exp \\left( -\\frac{1}{2} r^2 ; \\right) ; r ; dr ; \\right) ; d \\theta \\ \\ \u0026= \\int_0^{2 \\pi} 1 \\cdot d \\theta \\ \\ \u0026= 2 \\pi \\end{align*} $$\nAnd thus:\n$$ \\int_{-\\infty}^{\\infty} \\exp \\left( -\\frac{1}{2} x^2 \\right) ,dx = \\sqrt{2 \\pi} $$\nBy the law of substitution, the following equation also holds:\n$$ \\text{For any } a \u003e 0 \\text{ and } \\forall b \\in \\mathbb{R}, \\quad \\int_{-\\infty}^{\\infty} \\exp \\left( -\\frac{1}{2} \\frac{1}{a}(x - b)^2\\right) ; dx = \\sqrt{2 \\pi a} $$\nOr equivantly, written in the form of Gaussian distribution:\n$$ \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2 \\pi a}} \\exp \\left( -\\frac{(x - b)^2}{2 a} \\right) ; dx = 1 $$\nII. From Univariate to Multivariate We now generalize the univariate Gaussian Integral into multivariate cases.\nFirstly, clarify some notation:\n$$ \\begin{align*} \u0026\\text{(1)} \\quad \\boldsymbol{x} = \\begin{bmatrix} x_1 \\ \\vdots \\ x_n \\end{bmatrix}, \\boldsymbol{x} \\in \\mathbb{R}^n, \\quad \\text{where } x_i \\in \\mathbb{R}, \\quad i = 1, \\dots, n \\[1.5ex]\n\u0026\\text{(2)} \\quad d \\boldsymbol{x} ; \\text{ denotes the product measure } ; \\prod_{i=1}^n dx_i \\[1.5ex]\n\u0026\\text{(3)} \\quad \\text{Let } f: \\mathbb{R}^n \\to \\mathbb{R}. \\text{Then} \\int_{\\mathbb{R}^n} f(\\boldsymbol{x}) , d \\boldsymbol{x} = \\idotsint_{x_1, \\dots , x_n \\in \\mathbb{R}} f(x_1, \\dots, x_n) ,dx_1 \\cdots dx_n \\end{align*} $$\nThe statements are below:\n$$ \\begin{align*} \\text{Let } A \u0026\\in \\mathbb{R}^{n \\times n} \\text{ be symmetric and positive definite, and let } \\boldsymbol{b} \\in \\mathbb{R}^n, \\[2ex]\n\u0026\\int_{\\mathbb{R}^n} \\exp\\left( -\\frac{1}{2} (\\boldsymbol{x} - \\boldsymbol{b})^T A^{-1} (\\boldsymbol{x} - \\boldsymbol{b}) \\right) , d\\boldsymbol{x} = \\sqrt{ \\left(2 \\pi \\right) ^n \\cdot \\det (A) } \\end{align*} $$\nHere are steps to calculate it.\nStep1 Since $A$ is symmetric and positive definite matrix, thus $A^{-1}$ is also symmetric and positive definite matrix.\nThe Spectral Theorem guarantees that:\n$$ A^{-1} = Q^T \\Lambda Q, $$\nwhere $\\Lambda$ is a diagonal matrix with positive entries, and $Q$ is an orthogonal matrix(i.e., $Q^T Q = QQ^T = I$).\nThen transform the variable:\n$$ \\boldsymbol{y} = Q (\\boldsymbol{x} - \\boldsymbol{b}). $$\nOr, for the convenience of integration,\n$$ \\boldsymbol{y} = f(\\boldsymbol{x}) = \\begin{bmatrix} f_1(x_1, \\dots, x_n) \\ f_2(x_1, \\dots, x_n) \\ \\vdots \\ f_n(x_1, \\dots, x_n) \\end{bmatrix} = \\begin{bmatrix} y_1 \\ y_2 \\ \\vdots \\ y_n \\end{bmatrix} \\in \\mathbb{R}^n, $$\nwhere the corresponding Jacobian Matrix is:\n$$ \\mathrm{J} = \\frac{\\partial \\boldsymbol{y}}{\\partial \\boldsymbol{x}} = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} \u0026 \\cdots \u0026 \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026 \\ddots \u0026 \\vdots \\ \\frac{\\partial f_n}{\\partial x_1} \u0026 \\cdots \u0026 \\frac{\\partial f_n}{\\partial x_n} \\end{bmatrix} \\in \\mathbb{R}^{n \\times n}. $$\nSince $$y_i = \\sum_{j = 1}^{n} Q_{ij} (x_i - b_i)$$ and $$\\frac{\\partial f_i}{\\partial x_k} = \\frac{\\partial y_i}{\\partial x_k} = Q_{ik}$$ ,\n$$ \\mathrm{J}\n= \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} \u0026 \\cdots \u0026 \\frac{\\partial f_1}{\\partial x_n} \\ \\vdots \u0026 \\ddots \u0026 \\vdots \\ \\frac{\\partial f_n}{\\partial x_1} \u0026 \\cdots \u0026 \\frac{\\partial f_n}{\\partial x_n} \\end{bmatrix}\n= Q, \\quad \\text{and } \\det{(\\mathrm{J})} = \\det{(\\mathrm{Q})} = 1 $$\nSo that\n$$ d \\boldsymbol{y} = \\frac{\\partial(y_1,\\dots, y_n)}{\\partial(x_1, \\dots, x_n)} d \\boldsymbol{x}= \\det{(\\mathrm{J})} , d \\boldsymbol{x} = d \\boldsymbol{x}. $$\nTherefore,\n$$ \\begin{align*} \u0026\\int_{\\mathbb{R}^n} \\exp\\left( -\\frac{1}{2} (\\boldsymbol{x} - \\boldsymbol{b})^T A^{-1} (\\boldsymbol{x} - \\boldsymbol{b}) \\right) , d\\boldsymbol{x} \\[2ex] \u0026\\int_{\\mathbb{R}^n} \\exp \\left( -\\frac{1}{2} (\\boldsymbol{x} - \\boldsymbol{b})^T Q^T \\Lambda Q (\\boldsymbol{x} - \\boldsymbol{b}) \\right) , d \\boldsymbol{x} \\[2ex] \u0026\\int_{\\mathbb{R}^n} \\exp \\left( -\\frac{1}{2} \\left(Q (\\boldsymbol{x} - \\boldsymbol{b}) \\right)^T \\Lambda \\left(Q (\\boldsymbol{x} - \\boldsymbol{b}) \\right) \\right) , d \\boldsymbol{x} \\[2ex] \u0026\\int_{\\mathbb{R}^n} \\exp \\left( -\\frac{1}{2} \\boldsymbol{y}^T \\Lambda \\boldsymbol{y} \\right) , d \\boldsymbol{y}. \\end{align*} $$\nStep2 We focus on $\\boldsymbol{y}^T \\Lambda \\boldsymbol{y}$ :\n$$ \\boldsymbol{y}^T \\Lambda \\boldsymbol{y} \\begin{bmatrix} y_1 \u0026 y_2 \u0026 \\dots \u0026 y_n\\end{bmatrix}\n\\begin{bmatrix} \\lambda_1 \u0026 \\cdots \u0026 0 \\ \\vdots \u0026 \\ddots \u0026 \\vdots \\ 0 \u0026 \\cdots \u0026 \\lambda_n \\end{bmatrix}\n\\begin{bmatrix} y_1 \\ \\vdots \\ y_n\\end{bmatrix}\n= \\sum_{i=1}^{n}\\lambda_i y_i^2 $$\nSo ,\n$$ \\begin{align*} \u0026\\int_{\\mathbb{R}^n} \\exp \\left( -\\frac{1}{2} \\boldsymbol{y}^T \\Lambda \\boldsymbol{y} \\right) , d \\boldsymbol{y} \\[2ex] \u0026\\int_{\\mathbb{R}^n} \\exp \\left( -\\frac{1}{2} \\sum_{i=1}^{n}\\lambda_i y_i^2 \\right) , d \\boldsymbol{y} \\[2ex] \u0026\\idotsint_{y_1, \\dots , y_n \\in \\mathbb{R}} \\exp \\left( -\\frac{1}{2} \\sum_{i=1}^{n}\\lambda_i y_i^2 \\right) ,dy_1 \\cdots dy_n \\[2ex] \u0026\\idotsint_{y_1, \\dots , y_n \\in \\mathbb{R}} \\prod_{i=1}^{n} \\exp \\left( -\\frac{1}{2} \\lambda_i y_i^2 \\right) ,dy_1 \\cdots dy_n \\[2ex] \u0026\\prod_{i=1}^{n} \\int_{y_i \\in \\mathbb{R}}\\exp \\left( -\\frac{1}{2} \\lambda_i y_i^2 \\right) , d y_i \\end{align*} $$\nStep3 Using the result from univariate case, we obtain that:\n$$ \\prod_{i=1}^{n} \\int_{y_i \\in \\mathbb{R}}\\exp \\left( -\\frac{1}{2} \\lambda_i y_i^2 \\right) , d y_i \\prod_{i=1}^{n} \\sqrt{\\frac{2 \\pi}{\\lambda_i}} \\frac{\\left( \\sqrt{2 \\pi} \\right)^n}{\\sqrt{\\det (\\Lambda)}} $$\nSince\n$$ A^{-1} = Q^T \\Lambda Q, $$\nthen\n$$ \\begin{align*} \\quad \\det{(A^{-1})} \u0026= \\det{(Q^T)} \\det{(\\Lambda)} \\det{(Q)} \\[1.5ex] \u0026= \\det{(\\Lambda)} \\[1.5ex] \u0026= \\frac{1}{\\det{(A)}} \\end{align*} $$\nSo,\n$$ \\begin{align*} \u0026\\int_{\\mathbb{R}^n} \\exp\\left( -\\frac{1}{2} (\\boldsymbol{x} - \\boldsymbol{b})^T A (\\boldsymbol{x} - \\boldsymbol{b}) \\right) , d\\boldsymbol{x} \\[2ex] \u0026\\int_{\\mathbb{R}^n} \\exp \\left( -\\frac{1}{2} \\boldsymbol{y}^T \\Lambda \\boldsymbol{y} \\right) , d \\boldsymbol{y} \\[2ex] \u0026\\prod_{i=1}^{n} \\int_{y_i \\in \\mathbb{R}}\\exp \\left( -\\frac{1}{2} \\lambda_i y_i^2 \\right) , d y_i \\[2ex] \u0026\\frac{\\left( \\sqrt{2 \\pi} \\right)^n}{\\sqrt{\\det (\\Lambda)}} \\[2ex] \u0026\\sqrt{ \\left( 2 \\pi \\right) ^n \\cdot \\det (A) } \\end{align*} $$\nIII Probability \u0026 Linear Algebra Perspectives 1. Univariate Gaussian Distribution $$ \\begin{equation*} \\begin{aligned} \\text{Let a Gaussian random variable } X \u0026\\sim \\mathcal{N} (\\mu, , \\sigma^2), \\text{then:}\\ \\int_{-\\infty}^{\\infty} f_X(x) ; dx \u0026= \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( -\\frac{(x - \\mu)^2}{2 \\sigma^2} \\right) ; dx = 1 \\end{aligned} \\end{equation*} $$\nThis normalization condition is straightforward: the mean $\\mu$ and variance $\\sigma^2$ simply play the roles of shift and scaling parameters (analogous to $b$ and $$a$$ in standard variable transformations).\n2. Multivariate Gaussian Distribution Firstly, clarify some notation:\n$$ \\boldsymbol{X} = \\begin{bmatrix} X_1 \\ \\vdots \\ X_n \\end{bmatrix}, \\boldsymbol{X} \\in \\mathbb{R}^n, \\quad \\text{where } X_i \\in \\mathbb{R}, \\quad i = 1, \\dots, n $$\nThe bold and captial $\\boldsymbol{X}$ are random vector whose elements (i.e. Captial $X_i \\quad i = 1, \\dots, n$ )are random variables.\n$$ \\begin{equation*} \\begin{aligned} \\text{Let a Gaussian random vector } \\boldsymbol{X} \u0026\\sim \\mathcal{N} (\\boldsymbol{\\mu}, , \\Sigma), \\text{then:} \\ \\int_{\\mathbb{R}^n} f_{\\boldsymbol{X}}(\\boldsymbol{x}) ; d \\boldsymbol{x} \u0026= \\int_{\\mathbb{R}} \\frac{1}{\\sqrt{\\left(2\\pi \\right)^n \\cdot \\det(\\Sigma)}} \\exp \\left( -\\frac{1}{2} (\\boldsymbol{x}-\\boldsymbol{\\mu})^T \\Sigma^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu})\\right) ; d \\boldsymbol{x} = 1 \\end{aligned} \\end{equation*} $$\nWe focus on the variable transformation that simplifies the quadratic form in the exponent.\n(1) Diagonalizing $\\Sigma^{-1}$: PCA Perspective $$\\Sigma^{-1} = Q^T \\Lambda Q,$$\nwhere $$Q$$ is an orthogonal matrix whose rows are the eigenvectors of $\\Sigma^{-1}$, and $\\Lambda$ is a diagonal matrix of corresponding eigenvalues.\nThis procedure is exactly the core idea of Principal Component Analysis (i.e. PCA): identifying the principal axes (directions of maximal variance) of the distribution and aligning the coordinate system accordingly.\nAnd, the principal axed are uncorrelated.\n(2) Variable Transform We apply the transform of variable:\n$$ \\boldsymbol{y} = Q (\\boldsymbol{x} - \\boldsymbol{\\mu}), $$\nor equivalently,\n$$ \\boldsymbol{Y} = Q (\\boldsymbol{X} - \\boldsymbol{\\mu}). $$\nwhich rotates and centers the coordinate system. Under this transformation, the quadratic form becomes:\n$$ (\\boldsymbol{x} - \\boldsymbol{\\mu})^T \\Sigma^{-1} (\\boldsymbol{x} - \\boldsymbol{\\mu}) = \\boldsymbol{y}^T \\Lambda \\boldsymbol{y}, $$\nFrom $\\boldsymbol{x}$ to $\\boldsymbol{y}$, we decouple correlated $\\boldsymbol{x}$ into uncorrelated $\\boldsymbol{y}$ ($\\Lambda$ is the inverse of $\\mathrm{cov}(\\boldsymbol{y}^T \\boldsymbol{y})$, and is a diogonal matrix).\n(3) Uncorelation and Independence For general distributions, uncorrelated variables are not necessarily independent.\nHowever, in the case of multivariate gaussian, uncorrelatedness does imply independence!\nThis allows the joint distribution to be factored into a porduct of 1-D Gaussian densities:\n$$ f_{\\boldsymbol{Y}}(\\boldsymbol{y}) = \\prod_{i=1}^n \\mathcal{N}(y_i; 0, \\lambda_i^{-1}), $$\ngreatly simplifying the integral.\nThanks to Chat GPT. My Chinglish is childish and she helps to organise my words.\n","wordCount":"1506","inLanguage":"en","datePublished":"2025-05-19T00:00:00+08:00","dateModified":"2025-05-19T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://hanbinzheng.github.io/blog/gaussian-integral/"},"publisher":{"@type":"Organization","name":"Hanbin Zheng","logo":{"@type":"ImageObject","url":"https://hanbinzheng.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://hanbinzheng.github.io/ accesskey=h title="Hanbin Zheng (Alt + H)">Hanbin Zheng</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://hanbinzheng.github.io/ title=Home><span>Home</span></a></li><li><a href=https://hanbinzheng.github.io/academic title=Academic><span>Academic</span></a></li><li><a href=https://hanbinzheng.github.io/about title=About><span>About</span></a></li><li><a href=https://hanbinzheng.github.io/blog title=Blog><span>Blog</span></a></li><li><a href=https://hanbinzheng.github.io/archive title=Archive><span>Archive</span></a></li><li><a href=https://hanbinzheng.github.io/else title=Else><span>Else</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">The Gaussian Integral</h1><div class=post-meta><span title='2025-05-19 00:00:00 +0800 +0800'>May 19, 2025</span></div></header><div class=post-content><blockquote><p>This post introduces the Gaussian integral, starting from basic integration and gradually building towards a probabilistic and linear algebraic perspective.</p></blockquote><blockquote><p>The exposition proceeds from a purely mathematical derivation to a more intuitive understanding grounded in probability and linear algebra.</p></blockquote><h2 id=i-the-fundamental-theorem>I. The Fundamental Theorem<a hidden class=anchor aria-hidden=true href=#i-the-fundamental-theorem>#</a></h2><p>The most fundamental Gaussian Integral is as follows:</p><p>$$
\int_{-\infty}^{\infty} \exp \left(-\frac{1}{2}x^2 \right) dx = \sqrt{2\pi}
$$</p><p>Equivalently, in the form of the <strong>standard Gaussian distribution</strong>:</p><p>$$
\frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp \left( -\frac{1}{2} x^2 \right) dx = 1
$$</p><p>Here is how to compute this integral. For convenience, denote this integral by $\mathrm{I}$.</p><h3 id=step1>Step1<a hidden class=anchor aria-hidden=true href=#step1>#</a></h3><p>$$
\begin{align*}
\mathrm{I} &= \int_{-\infty}^{\infty} \exp \left( -\frac{1}{2}x^2 \right) dx \ \
\mathrm{I}^2 &= \int_{-\infty}^{\infty} \exp \left( -\frac{1}{2} x^2 \right) dx ; \cdot ; \int_{-\infty}^{\infty} \exp \left( -\frac{1}{2} y^2 \right) dy \ \
&= \iint_{\mathbb{R}^2} \exp \left( -\frac{1}{2}(x^2 + y^2) \right) dx ; dy
\end{align*}
$$</p><hr><h3 id=step2>Step2<a hidden class=anchor aria-hidden=true href=#step2>#</a></h3><p>Transform from <strong>Cartesian Coordinates</strong> to <strong>Polar Coordinates</strong>, where</p><p>$$
\left {
\begin{align*}
x &= r ; \cos (\theta) \
y &= r ; \sin (\theta)
\end{align*}
\right.
$$</p><p>And the <strong>Jacobian matrix</strong> for this transform is:</p><p>$$
\mathrm{J}(r, \theta) = \frac{\partial (x, y)}{\partial (r, \theta)} =\begin{bmatrix}
\frac{\partial x}{\partial r} & \frac{\partial x}{\partial \theta} \
\frac{\partial y}{\partial r} & \frac{\partial y}{\partial \theta}
\end{bmatrix}
= \begin{bmatrix}
\cos (\theta) & - r \sin (\theta) \
\sin (\theta) & r \cos (\theta)
\end{bmatrix}
$$</p><p>and the transform from $dx ; dy$$ to $$dr ; d \theta$ is:</p><p>$$
\begin{align*}
dx ; dy &= \det \left( \mathrm{J}(r, \theta) \right) dr ; d \theta \ \
&= r \left( \cos^2 (\theta) + \sin^2 (\theta) \right) ;dr ; d \theta\ \
&= r ; dr ; d \theta
\end{align*}
$$</p><hr><h3 id=step3>Step3<a hidden class=anchor aria-hidden=true href=#step3>#</a></h3><p>So after the transform, the equation for integral $\mathrm{I}$ is:</p><p>$$
\begin{align*}
\mathrm{I}^2 &= \iint_{\mathbb{R}^2}\exp \left( -\frac{1}{2} r^2 \right) ; r ; dr ; d \theta \ \
&= \int_0^{2 \pi} \left( \int_0^{\infty} \exp \left( -\frac{1}{2} r^2 ; \right) ; r ; dr ; \right) ; d \theta \ \
&= \int_0^{2 \pi} 1 \cdot d \theta \ \
&= 2 \pi
\end{align*}
$$</p><p>And thus:</p><p>$$
\int_{-\infty}^{\infty} \exp \left( -\frac{1}{2} x^2 \right) ,dx = \sqrt{2 \pi}
$$</p><p>By the law of substitution, the following equation also holds:</p><p>$$
\text{For any } a > 0 \text{ and } \forall b \in \mathbb{R}, \quad \int_{-\infty}^{\infty} \exp \left( -\frac{1}{2} \frac{1}{a}(x - b)^2\right) ; dx = \sqrt{2 \pi a}
$$</p><p>Or equivantly, written in the form of Gaussian distribution:</p><p>$$
\int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi a}}
\exp \left( -\frac{(x - b)^2}{2 a} \right) ; dx = 1
$$</p><hr><h2 id=ii-from-univariate-to-multivariate>II. From Univariate to Multivariate<a hidden class=anchor aria-hidden=true href=#ii-from-univariate-to-multivariate>#</a></h2><p>We now generalize the <strong>univariate Gaussian Integral</strong> into <strong>multivariate</strong> cases.</p><p>Firstly, clarify some notation:</p><p>$$
\begin{align*}
&\text{(1)} \quad
\boldsymbol{x} = \begin{bmatrix} x_1 \ \vdots \ x_n \end{bmatrix}, \boldsymbol{x}
\in \mathbb{R}^n, \quad
\text{where } x_i \in \mathbb{R}, \quad i = 1, \dots, n \[1.5ex]</p><p>&\text{(2)} \quad
d \boldsymbol{x} ; \text{ denotes the product measure } ;
\prod_{i=1}^n dx_i \[1.5ex]</p><p>&\text{(3)} \quad
\text{Let } f: \mathbb{R}^n \to \mathbb{R}.
\text{Then} \int_{\mathbb{R}^n} f(\boldsymbol{x}) , d \boldsymbol{x}
= \idotsint_{x_1, \dots , x_n \in \mathbb{R}} f(x_1, \dots, x_n) ,dx_1 \cdots dx_n
\end{align*}
$$</p><p>The statements are below:</p><p>$$
\begin{align*}
\text{Let } A &\in \mathbb{R}^{n \times n} \text{ be symmetric and positive definite, and let } \boldsymbol{b} \in \mathbb{R}^n, \[2ex]</p><p>&\int_{\mathbb{R}^n}
\exp\left(
-\frac{1}{2} (\boldsymbol{x} - \boldsymbol{b})^T A^{-1} (\boldsymbol{x} - \boldsymbol{b})
\right)
, d\boldsymbol{x}
= \sqrt{ \left(2 \pi \right) ^n \cdot \det (A) }
\end{align*}
$$</p><p>Here are steps to calculate it.</p><hr><h3 id=step1-1>Step1<a hidden class=anchor aria-hidden=true href=#step1-1>#</a></h3><p>Since $A$ is <strong>symmetric and positive definite</strong> matrix, thus $A^{-1}$ is also <strong>symmetric and positive definite</strong> matrix.</p><p>The <a href=https://math.mit.edu/~dav/spectral.pdf>Spectral Theorem</a> guarantees that:</p><p>$$
A^{-1} = Q^T \Lambda Q,
$$</p><p>where $\Lambda$ is a <strong>diagonal matrix</strong> with positive entries, and $Q$ is an <strong>orthogonal matrix</strong>(i.e., $Q^T Q = QQ^T = I$).</p><p>Then transform the variable:</p><p>$$
\boldsymbol{y} = Q (\boldsymbol{x} - \boldsymbol{b}).
$$</p><p>Or, for the convenience of integration,</p><p>$$
\boldsymbol{y} = f(\boldsymbol{x}) =
\begin{bmatrix}
f_1(x_1, \dots, x_n) \
f_2(x_1, \dots, x_n) \
\vdots \
f_n(x_1, \dots, x_n)
\end{bmatrix}
= \begin{bmatrix}
y_1 \
y_2 \
\vdots \
y_n
\end{bmatrix}
\in \mathbb{R}^n,
$$</p><p>where the corresponding <strong>Jacobian Matrix</strong> is:</p><p>$$
\mathrm{J} = \frac{\partial \boldsymbol{y}}{\partial \boldsymbol{x}} =
\begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \
\vdots & \ddots & \vdots \
\frac{\partial f_n}{\partial x_1} & \cdots & \frac{\partial f_n}{\partial x_n}
\end{bmatrix}
\in \mathbb{R}^{n \times n}.
$$</p><p>Since $$y_i = \sum_{j = 1}^{n} Q_{ij} (x_i - b_i)$$ and $$\frac{\partial f_i}{\partial x_k} = \frac{\partial y_i}{\partial x_k} = Q_{ik}$$ ,</p><p>$$
\mathrm{J}</p><p>= \begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \
\vdots & \ddots & \vdots \
\frac{\partial f_n}{\partial x_1} & \cdots & \frac{\partial f_n}{\partial x_n}
\end{bmatrix}</p><p>= Q, \quad \text{and } \det{(\mathrm{J})} = \det{(\mathrm{Q})} = 1
$$</p><p>So that</p><p>$$
d \boldsymbol{y} = \frac{\partial(y_1,\dots, y_n)}{\partial(x_1, \dots, x_n)} d \boldsymbol{x}= \det{(\mathrm{J})} , d \boldsymbol{x} = d \boldsymbol{x}.
$$</p><p>Therefore,</p><h1 id=int_mathbbrn-expleft--frac12-boldsymbolx---boldsymbolbt-a-1-boldsymbolx---boldsymbolb-right--dboldsymbolx-2ex>$$
\begin{align*}
&\int_{\mathbb{R}^n} \exp\left( -\frac{1}{2} (\boldsymbol{x} - \boldsymbol{b})^T A^{-1} (\boldsymbol{x} - \boldsymbol{b}) \right) , d\boldsymbol{x} \[2ex]</h1><h1 id=int_mathbbrn-exp-left--frac12-boldsymbolx---boldsymbolbt-qt-lambda-q-boldsymbolx---boldsymbolb-right--d-boldsymbolx-2ex>&\int_{\mathbb{R}^n} \exp \left( -\frac{1}{2} (\boldsymbol{x} - \boldsymbol{b})^T Q^T \Lambda Q (\boldsymbol{x} - \boldsymbol{b}) \right) , d \boldsymbol{x} \[2ex]<a hidden class=anchor aria-hidden=true href=#int_mathbbrn-exp-left--frac12-boldsymbolx---boldsymbolbt-qt-lambda-q-boldsymbolx---boldsymbolb-right--d-boldsymbolx-2ex>#</a></h1><h1 id=int_mathbbrn-exp-left--frac12-leftq-boldsymbolx---boldsymbolb-rightt-lambda-leftq-boldsymbolx---boldsymbolb-right-right--d-boldsymbolx-2ex>&\int_{\mathbb{R}^n} \exp \left( -\frac{1}{2} \left(Q (\boldsymbol{x} - \boldsymbol{b}) \right)^T \Lambda \left(Q (\boldsymbol{x} - \boldsymbol{b}) \right) \right) , d \boldsymbol{x} \[2ex]<a hidden class=anchor aria-hidden=true href=#int_mathbbrn-exp-left--frac12-leftq-boldsymbolx---boldsymbolb-rightt-lambda-leftq-boldsymbolx---boldsymbolb-right-right--d-boldsymbolx-2ex>#</a></h1><p>&\int_{\mathbb{R}^n} \exp \left( -\frac{1}{2} \boldsymbol{y}^T \Lambda \boldsymbol{y} \right) , d \boldsymbol{y}.
\end{align*}
$$</p><hr><h3 id=step2-1>Step2<a hidden class=anchor aria-hidden=true href=#step2-1>#</a></h3><p>We focus on $\boldsymbol{y}^T \Lambda \boldsymbol{y}$ :</p><h1 id=boldsymbolyt-lambda-boldsymboly>$$
\boldsymbol{y}^T \Lambda \boldsymbol{y}</h1><p>\begin{bmatrix} y_1 & y_2 & \dots & y_n\end{bmatrix}</p><p>\begin{bmatrix}
\lambda_1 & \cdots & 0 \
\vdots & \ddots & \vdots \
0 & \cdots & \lambda_n
\end{bmatrix}</p><p>\begin{bmatrix} y_1 \ \vdots \ y_n\end{bmatrix}</p><p>= \sum_{i=1}^{n}\lambda_i y_i^2
$$</p><p>So ,</p><h1 id=int_mathbbrn-exp-left--frac12-boldsymbolyt-lambda-boldsymboly-right--d-boldsymboly-2ex>$$
\begin{align*}
&\int_{\mathbb{R}^n} \exp \left( -\frac{1}{2} \boldsymbol{y}^T \Lambda \boldsymbol{y} \right) , d \boldsymbol{y} \[2ex]</h1><h1 id=int_mathbbrn-exp-left--frac12-sum_i1nlambda_i-y_i2-right--d-boldsymboly-2ex>&\int_{\mathbb{R}^n} \exp \left( -\frac{1}{2} \sum_{i=1}^{n}\lambda_i y_i^2 \right) , d \boldsymbol{y} \[2ex]<a hidden class=anchor aria-hidden=true href=#int_mathbbrn-exp-left--frac12-sum_i1nlambda_i-y_i2-right--d-boldsymboly-2ex>#</a></h1><h1 id=idotsint_y_1-dots--y_n-in-mathbbr--exp-left--frac12-sum_i1nlambda_i-y_i2-right-dy_1-cdots-dy_n-2ex>&\idotsint_{y_1, \dots , y_n \in \mathbb{R}} \exp \left( -\frac{1}{2} \sum_{i=1}^{n}\lambda_i y_i^2 \right) ,dy_1 \cdots dy_n \[2ex]<a hidden class=anchor aria-hidden=true href=#idotsint_y_1-dots--y_n-in-mathbbr--exp-left--frac12-sum_i1nlambda_i-y_i2-right-dy_1-cdots-dy_n-2ex>#</a></h1><h1 id=idotsint_y_1-dots--y_n-in-mathbbr-prod_i1n-exp-left--frac12-lambda_i-y_i2-right-dy_1-cdots-dy_n-2ex>&\idotsint_{y_1, \dots , y_n \in \mathbb{R}} \prod_{i=1}^{n} \exp \left( -\frac{1}{2} \lambda_i y_i^2 \right) ,dy_1 \cdots dy_n \[2ex]<a hidden class=anchor aria-hidden=true href=#idotsint_y_1-dots--y_n-in-mathbbr-prod_i1n-exp-left--frac12-lambda_i-y_i2-right-dy_1-cdots-dy_n-2ex>#</a></h1><p>&\prod_{i=1}^{n} \int_{y_i \in \mathbb{R}}\exp \left( -\frac{1}{2} \lambda_i y_i^2 \right) , d y_i
\end{align*}
$$</p><hr><h3 id=step3-1>Step3<a hidden class=anchor aria-hidden=true href=#step3-1>#</a></h3><p>Using the result from univariate case, we obtain that:</p><h1 id=prod_i1n-int_y_i-in-mathbbrexp-left--frac12-lambda_i-y_i2-right--d-y_i>$$
\prod_{i=1}^{n} \int_{y_i \in \mathbb{R}}\exp \left( -\frac{1}{2} \lambda_i y_i^2 \right) , d y_i</h1><h1 id=prod_i1n-sqrtfrac2-pilambda_i>\prod_{i=1}^{n} \sqrt{\frac{2 \pi}{\lambda_i}}<a hidden class=anchor aria-hidden=true href=#prod_i1n-sqrtfrac2-pilambda_i>#</a></h1><p>\frac{\left( \sqrt{2 \pi} \right)^n}{\sqrt{\det (\Lambda)}}
$$</p><p>Since</p><p>$$
A^{-1} = Q^T \Lambda Q,
$$</p><p>then</p><p>$$
\begin{align*}
\quad \det{(A^{-1})}
&= \det{(Q^T)} \det{(\Lambda)} \det{(Q)} \[1.5ex]
&= \det{(\Lambda)} \[1.5ex]
&= \frac{1}{\det{(A)}}
\end{align*}
$$</p><p>So,</p><h1 id=int_mathbbrn-expleft--frac12-boldsymbolx---boldsymbolbt-a-boldsymbolx---boldsymbolb-right--dboldsymbolx-2ex>$$
\begin{align*}
&\int_{\mathbb{R}^n} \exp\left( -\frac{1}{2} (\boldsymbol{x} - \boldsymbol{b})^T A (\boldsymbol{x} - \boldsymbol{b}) \right) , d\boldsymbol{x} \[2ex]</h1><h1 id=int_mathbbrn-exp-left--frac12-boldsymbolyt-lambda-boldsymboly-right--d-boldsymboly-2ex-1>&\int_{\mathbb{R}^n} \exp \left( -\frac{1}{2} \boldsymbol{y}^T \Lambda \boldsymbol{y} \right) , d \boldsymbol{y} \[2ex]<a hidden class=anchor aria-hidden=true href=#int_mathbbrn-exp-left--frac12-boldsymbolyt-lambda-boldsymboly-right--d-boldsymboly-2ex-1>#</a></h1><h1 id=prod_i1n-int_y_i-in-mathbbrexp-left--frac12-lambda_i-y_i2-right--d-y_i--2ex>&\prod_{i=1}^{n} \int_{y_i \in \mathbb{R}}\exp \left( -\frac{1}{2} \lambda_i y_i^2 \right) , d y_i \[2ex]<a hidden class=anchor aria-hidden=true href=#prod_i1n-int_y_i-in-mathbbrexp-left--frac12-lambda_i-y_i2-right--d-y_i--2ex>#</a></h1><h1 id=fracleft-sqrt2-pi-rightnsqrtdet-lambda-2ex>&\frac{\left( \sqrt{2 \pi} \right)^n}{\sqrt{\det (\Lambda)}} \[2ex]<a hidden class=anchor aria-hidden=true href=#fracleft-sqrt2-pi-rightnsqrtdet-lambda-2ex>#</a></h1><p>&\sqrt{ \left( 2 \pi \right) ^n \cdot \det (A) }
\end{align*}
$$</p><hr><h2 id=iii-probability--linear-algebra-perspectives>III Probability & Linear Algebra Perspectives<a hidden class=anchor aria-hidden=true href=#iii-probability--linear-algebra-perspectives>#</a></h2><h3 id=1-univariate-gaussian-distribution>1. Univariate Gaussian Distribution<a hidden class=anchor aria-hidden=true href=#1-univariate-gaussian-distribution>#</a></h3><p>$$
\begin{equation*}
\begin{aligned}
\text{Let a Gaussian random variable } X &\sim \mathcal{N} (\mu, , \sigma^2), \text{then:}\
\int_{-\infty}^{\infty} f_X(x) ; dx &= \int_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi \sigma^2}}
\exp \left( -\frac{(x - \mu)^2}{2 \sigma^2} \right) ; dx = 1
\end{aligned}
\end{equation*}
$$</p><p>This normalization condition is straightforward: the mean $\mu$ and variance $\sigma^2$ simply play the roles of shift and scaling parameters (analogous to $b$ and $$a$$ in standard variable transformations).</p><hr><h3 id=2-multivariate-gaussian-distribution>2. Multivariate Gaussian Distribution<a hidden class=anchor aria-hidden=true href=#2-multivariate-gaussian-distribution>#</a></h3><p>Firstly, clarify some notation:</p><p>$$
\boldsymbol{X} = \begin{bmatrix} X_1 \ \vdots \ X_n \end{bmatrix}, \boldsymbol{X}
\in \mathbb{R}^n, \quad
\text{where } X_i \in \mathbb{R}, \quad i = 1, \dots, n
$$</p><p>The <strong>bold</strong> and captial $\boldsymbol{X}$ are <strong>random vector</strong> whose elements (i.e. <strong>Captial</strong> $X_i \quad i = 1, \dots, n$ )are <strong>random variables</strong>.</p><p>$$
\begin{equation*}
\begin{aligned}
\text{Let a Gaussian random vector } \boldsymbol{X} &\sim \mathcal{N} (\boldsymbol{\mu}, , \Sigma), \text{then:} \
\int_{\mathbb{R}^n} f_{\boldsymbol{X}}(\boldsymbol{x}) ; d \boldsymbol{x} &= \int_{\mathbb{R}} \frac{1}{\sqrt{\left(2\pi \right)^n \cdot \det(\Sigma)}}
\exp \left( -\frac{1}{2} (\boldsymbol{x}-\boldsymbol{\mu})^T \Sigma^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\right) ; d \boldsymbol{x} = 1
\end{aligned}
\end{equation*}
$$</p><p>We focus on the variable transformation that simplifies the quadratic form in the exponent.</p><hr><h4 id=1-diagonalizing-sigma-1-pca-perspective>(1) <strong>Diagonalizing $\Sigma^{-1}$: PCA Perspective</strong><a hidden class=anchor aria-hidden=true href=#1-diagonalizing-sigma-1-pca-perspective>#</a></h4><p>$$\Sigma^{-1} = Q^T \Lambda Q,$$</p><p>where $$Q$$ is an orthogonal matrix whose rows are the eigenvectors of $\Sigma^{-1}$, and $\Lambda$ is a diagonal matrix of corresponding eigenvalues.</p><p>This procedure is exactly the core idea of <a href=https://www.cs.cmu.edu/~elaw/papers/pca.pdf>Principal Component Analysis</a> (i.e. <strong>PCA</strong>): identifying the principal axes (directions of maximal variance) of the distribution and aligning the coordinate system accordingly.</p><p>And, the principal axed are <strong>uncorrelated</strong>.</p><hr><h4 id=2-variable-transform>(2) <strong>Variable Transform</strong><a hidden class=anchor aria-hidden=true href=#2-variable-transform>#</a></h4><p>We apply the transform of variable:</p><p>$$
\boldsymbol{y} = Q (\boldsymbol{x} - \boldsymbol{\mu}),
$$</p><p>or equivalently,</p><p>$$
\boldsymbol{Y} = Q (\boldsymbol{X} - \boldsymbol{\mu}).
$$</p><p>which rotates and centers the coordinate system. Under this transformation, the quadratic form becomes:</p><p>$$
(\boldsymbol{x} - \boldsymbol{\mu})^T \Sigma^{-1} (\boldsymbol{x} - \boldsymbol{\mu}) = \boldsymbol{y}^T \Lambda \boldsymbol{y},
$$</p><p>From $\boldsymbol{x}$ to $\boldsymbol{y}$, we decouple correlated $\boldsymbol{x}$ into uncorrelated $\boldsymbol{y}$ ($\Lambda$ is the inverse of $\mathrm{cov}(\boldsymbol{y}^T \boldsymbol{y})$, and is a diogonal matrix).</p><hr><h4 id=3-uncorelation-and-independence>(3) <strong>Uncorelation and Independence</strong><a hidden class=anchor aria-hidden=true href=#3-uncorelation-and-independence>#</a></h4><p>For general distributions, uncorrelated variables are not necessarily independent.</p><p>However, <strong>in the case of multivariate gaussian, uncorrelatedness does imply independence!</strong></p><p>This allows the joint distribution to be factored into a porduct of 1-D Gaussian densities:</p><p>$$
f_{\boldsymbol{Y}}(\boldsymbol{y}) = \prod_{i=1}^n \mathcal{N}(y_i; 0, \lambda_i^{-1}),
$$</p><p>greatly simplifying the integral.</p><hr><blockquote><p>Thanks to Chat GPT. My Chinglish is childish and she helps to organise my words.</p></blockquote></div><footer class=post-footer><ul class=post-tags><li><a href=https://hanbinzheng.github.io/tags/math/>Math</a></li><li><a href=https://hanbinzheng.github.io/tags/probability/>Probability</a></li><li><a href=https://hanbinzheng.github.io/tags/pca/>PCA</a></li><li><a href=https://hanbinzheng.github.io/tags/gaussian/>Gaussian</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://hanbinzheng.github.io/>Hanbin Zheng</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>
<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>The Central Limit Theorem | Hanbin Zheng</title><meta name=keywords content="Probability,Math,CLT,Gaussian"><meta name=description content="
Thanks to 子预. His article helped me a lot in understanding this topic.
This is a brief (but not rigorous) proof of the Central Limit Theorem (CLT).
The CLT is one of the most fundamental results in probability theory, showing that the sum of many independent random variables tends toward a normal distribution, regardless of their original distribution.
Intuitively, the CLT explains why many natural phenomena follow the bell curve: individual randomness averages out into a predictable pattern."><meta name=author content><link rel=canonical href=https://hanbinzheng.github.io/blog/central-limit-theorem/><link crossorigin=anonymous href=/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css integrity="sha256-j+ECM6cGvIfy4Is8+XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as=style><link rel=icon href=https://hanbinzheng.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://hanbinzheng.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://hanbinzheng.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://hanbinzheng.github.io/apple-touch-icon.png><link rel=mask-icon href=https://hanbinzheng.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://hanbinzheng.github.io/blog/central-limit-theorem/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],packages:{"[+]":["ams"]}},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js defer></script><meta property="og:url" content="https://hanbinzheng.github.io/blog/central-limit-theorem/"><meta property="og:site_name" content="Hanbin Zheng"><meta property="og:title" content="The Central Limit Theorem"><meta property="og:description" content=" Thanks to 子预. His article helped me a lot in understanding this topic.
This is a brief (but not rigorous) proof of the Central Limit Theorem (CLT).
The CLT is one of the most fundamental results in probability theory, showing that the sum of many independent random variables tends toward a normal distribution, regardless of their original distribution.
Intuitively, the CLT explains why many natural phenomena follow the bell curve: individual randomness averages out into a predictable pattern."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-05-15T00:00:00+08:00"><meta property="article:modified_time" content="2025-05-15T00:00:00+08:00"><meta property="article:tag" content="Probability"><meta property="article:tag" content="Math"><meta property="article:tag" content="CLT"><meta property="article:tag" content="Gaussian"><meta name=twitter:card content="summary"><meta name=twitter:title content="The Central Limit Theorem"><meta name=twitter:description content="
Thanks to 子预. His article helped me a lot in understanding this topic.
This is a brief (but not rigorous) proof of the Central Limit Theorem (CLT).
The CLT is one of the most fundamental results in probability theory, showing that the sum of many independent random variables tends toward a normal distribution, regardless of their original distribution.
Intuitively, the CLT explains why many natural phenomena follow the bell curve: individual randomness averages out into a predictable pattern."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://hanbinzheng.github.io/blog/"},{"@type":"ListItem","position":2,"name":"The Central Limit Theorem","item":"https://hanbinzheng.github.io/blog/central-limit-theorem/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"The Central Limit Theorem","name":"The Central Limit Theorem","description":" Thanks to 子预. His article helped me a lot in understanding this topic.\nThis is a brief (but not rigorous) proof of the Central Limit Theorem (CLT).\nThe CLT is one of the most fundamental results in probability theory, showing that the sum of many independent random variables tends toward a normal distribution, regardless of their original distribution.\nIntuitively, the CLT explains why many natural phenomena follow the bell curve: individual randomness averages out into a predictable pattern.\n","keywords":["Probability","Math","CLT","Gaussian"],"articleBody":" Thanks to 子预. His article helped me a lot in understanding this topic.\nThis is a brief (but not rigorous) proof of the Central Limit Theorem (CLT).\nThe CLT is one of the most fundamental results in probability theory, showing that the sum of many independent random variables tends toward a normal distribution, regardless of their original distribution.\nIntuitively, the CLT explains why many natural phenomena follow the bell curve: individual randomness averages out into a predictable pattern.\nFor application of CLT and some advanced understanding, I will add this part if available.\nThe CLT Statement Let $$X_1, X_2, \\dots, X_n$$ be i.i.d. (independent and identically distributed) random variables with $\\mathbb{E}(X_j) = \\mu$$ and $$\\mathrm{Var}(X_j) = \\sigma^2 \u003c \\infty$.\nDefine:\n$$ Z_n = \\frac{\\sum_{j=1}^n X_j - n\\mu}{\\sqrt{n} \\cdot \\sigma} $$\nThen for all $\\tau \\in \\mathbb{R}$,\n$$ \\lim_{n \\to \\infty} \\mathbb{P}(Z_n \\leq \\tau) = \\Phi(\\tau) $$\nOr equivalently,\n$$ Z_n \\xrightarrow{D} \\mathcal{N}(0, 1) $$\nProof For Central Limit Theorem Notes This proof is sometimes called Lindeberg–Lévy CLT, requiring i.i.d. variables with finite variance. A more general version (Lindeberg–Feller CLT) loosens these assumptions. 1. The Road Map To ensure both clarity and completeness, let’s first outline the proof strategy. We will use the characteristic function method to prove the CLT. The ideas are simple:\nThere’s a one-to-one mapping relationship between characteristic function and pdf/distribution. If the characteristic functions of $$Z_n$$ and $\\mathcal{N}(0,1)$ are equal for all $t$, then their distributions are identical, which completes the proof. For the one-to-one mapping relationship, we should turn to Fourier Transform. I will add this part if available in the future. In this blog, we assume that the previous relationship holds. For the definition and some properties of Characteristic Function, see Appendix for reference.\n2. Characteristic Function of $\\mathcal{N}(0, 1)$ The definition of Characteristic Function for $\\text{ random variable } X ,$ is defined as follows:\n$$ \\varphi_X(t) = \\mathbb{E}[e^{itX}] $$\nFor standard Gaussian $\\text{ random variable } N \\sim \\mathcal{N}(0, 1)$, we can obtain the Characteristic Function below:\n$$ \\begin{align*} \\varphi_N(t) \u0026= \\int_{- \\infty}^{+ \\infty} e^{itx} \\cdot ; \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{1}{2} x^2} dx \\[2em] \u0026= \\frac{1}{\\sqrt{2 \\pi}} \\int_{- \\infty}^{+ \\infty} e^{-\\frac{1}{2}x^2 , + itx} dx \\ \\ \u0026= \\frac{1}{\\sqrt{2 \\pi}} \\int_{- \\infty}^{+ \\infty} e^{- \\frac{1}{2}(x - it)^2 + \\frac{1}{2}(it)^2} dx \\ \\ \u0026= e^{- \\frac{1}{2}t^2} \\left( \\frac{1}{\\sqrt{2 \\pi}} \\int_{- \\infty}^{+\\infty} e^{- \\frac{1}{2}(x - it)^2}d(x-it) \\right) \\ \\ \u0026= e^{- \\frac{1}{2} t^2} \\end{align*} $$\nTo evaluate the integral, we complete the square in the exponent.\nFor $\\frac{1}{\\sqrt{2 \\pi}} \\int_{- \\infty}^{+\\infty} e^{- \\frac{1}{2}(x - it)^2}d(x-it)$, the resulting integral is the total probability under the standard normal distribution (after a complex shift), and therefore equals 1.\n3. Characteristic Function of $$Z_n$$ Step 1: Standardize Variables Let’s define standardized variables:\n$$ Y_j = \\frac{X_j - \\mu}{\\sigma}, \\quad \\text{so that } \\mathbb{E}[Y_j] = 0, \\quad \\mathrm{Var}(Y_j) = 1 $$\nThen:\n$$ Z_n = \\frac{1}{\\sqrt{n}} \\sum_{j=1}^n Y_j $$\nThis reformulates the normalized sum into a simpler form, preparing it for the characteristic function method.\nStep 2: Characteristic Function of $$Y_j$$ The characteristic function of $Y_j$ is:\n$$ \\varphi_{Y_j}(t) = \\mathbb{E}[e^{itY_j}] $$\nUsing Taylor expansion:\n$$ \\begin{align*} \\varphi_{Y_j}(t) \u0026= \\mathbb{E}[\\sum_{k=0}^{\\infty} \\frac{(itY_j)^k}{k !}]\\ \\ \u0026= \\sum_{k=0}^{\\infty} \\frac{(it)^k}{k!} \\mathbb{E}[Y_j^k] \\ \\ \u0026= 1 - \\frac{1}{2} t^2 + o(t^2) \\ \\end{align*} $$\nStep 3: Characteristic Function of $$Z_n$$ Here are two properties of Characteristic Function:\nLinearity: $\\text{ Linearity}: ;\\varphi_{aX + b}(t) = e^{ibt} \\cdot \\varphi_X(at)$ Independence: $\\text{If random variable } X \\text{ and } Y \\text{ are independent }, ; \\varphi_{X + Y}(t) = \\varphi_X(t) \\cdot \\varphi_Y(t)$ The proof for this two properites can be seen in the Appendix.\n$$ \\begin{align*} \\varphi_{Z_n}(t) \u0026= \\varphi_{\\frac{1}{\\sqrt{n}} \\sum_j Y_j}(t) \\ \\ \u0026= \\left( \\varphi_{Y_j} \\left( \\frac{t}{\\sqrt{n}} \\right) \\right)^n \\ \\ \u0026= \\left(1 - \\frac{t^2}{2n} + o\\left( \\frac{t^2}{n} \\right) \\right)^n \\ \\end{align*} $$\nAs $n \\to \\infty$, by the exponential limit identity:\n$$ \\lim_{n \\to \\infty} \\varphi_{Z_n}(t) = \\lim_{n \\to \\infty} \\left(1 - \\frac{t^2}{2n} + o\\left(\\frac{1}{n}\\right)\\right)^n = e^{- \\frac{1}{2} t^2} $$\n4. Final Step $$ \\lim_{n \\to \\infty} \\varphi_{Z_n}(t) = \\varphi_{N}(t) \\quad \\Rightarrow \\quad Z_n \\xrightarrow{D} \\mathcal{N}(0, 1) \\quad \\blacksquare $$\nThis completes the proof of the Central Limit Theorem under the assumptions of i.i.d and finite variance.\nAppendix: Characteristic Functions We only use 2 properities of Characteristic Functions. For more detailed description, please check [this article] (I will write this if available).\nLinearity: $\\varphi_{aX + b}(t) = e^{ibt} \\cdot \\varphi_X(at)$ Independence: $\\text{If random variable } X \\text{ and } Y \\text{ are independent }, ; \\varphi_{X + Y}(t) = \\varphi_X(t) \\cdot \\varphi_Y(t)$ $\\text{Proof for Linearity}$:\n$$ \\varphi_{aX+b}(t) = \\mathbb{E}[e^{it(aX+b)}] = e^{itb} \\mathbb{E}[e^{itaX}] =e^{itb} \\varphi_X(at) $$\n$$\\text{Proof for Independence}$$:\nThis property is a consequence of the factorization of the joint distribution for independent random variables.\n$$ \\text{Suppose } \\alpha(X) \\text{ and } \\beta(Y) \\text{ are two functions of X and Y.} $$\n$$ \\begin{align*} \\mathbb{E}[\\alpha(X)\\beta(Y)] \u0026= \\iint_{D_{X, Y}} f_{X, Y}(x, y) \\cdot \\alpha(x) \\beta(y) ;dx , dy \\ \u0026= \\iint_{D_{X, Y}} f_X(x) , f_Y(y) \\cdot \\alpha(x) \\beta(y) ;dx , dy \\ \u0026= \\int_{D_X} f_X(x) , \\alpha(x) , dx ; \\cdot ; \\int_{D_Y} f_Y(y), \\beta(y) , dy \\ \u0026= \\mathbb{E}[\\alpha(X)] , \\cdot , \\mathbb{E}[\\beta(Y)] \\end{align*} $$\n$$ \\begin{aligned} \\varphi_{X + Y}(t) \u0026= \\mathbb{E}[e^{it(X + Y)}] \\ \\ \u0026= \\mathbb{E}[e^{itX} e^{itY}] \\ \\ \u0026= \\mathbb{E}[e^{itX}] , \\mathbb{E}[e^{itY}] \\ \\ \u0026= \\varphi_X(t) , \\varphi_Y(t) \\end{aligned} $$\n","wordCount":"875","inLanguage":"en","datePublished":"2025-05-15T00:00:00+08:00","dateModified":"2025-05-15T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://hanbinzheng.github.io/blog/central-limit-theorem/"},"publisher":{"@type":"Organization","name":"Hanbin Zheng","logo":{"@type":"ImageObject","url":"https://hanbinzheng.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://hanbinzheng.github.io/ accesskey=h title="Hanbin Zheng (Alt + H)">Hanbin Zheng</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://hanbinzheng.github.io/ title=Home><span>Home</span></a></li><li><a href=https://hanbinzheng.github.io/academic title=Academic><span>Academic</span></a></li><li><a href=https://hanbinzheng.github.io/about title=About><span>About</span></a></li><li><a href=https://hanbinzheng.github.io/blog title=Blog><span>Blog</span></a></li><li><a href=https://hanbinzheng.github.io/archive title=Archive><span>Archive</span></a></li><li><a href=https://hanbinzheng.github.io/else title=Else><span>Else</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">The Central Limit Theorem</h1><div class=post-meta><span title='2025-05-15 00:00:00 +0800 +0800'>May 15, 2025</span></div></header><div class=post-content><blockquote><p>Thanks to <a href=https://www.zhihu.com/people/thegenius-16>子预</a>. <a href=https://zhuanlan.zhihu.com/p/85233692>His article</a> helped me a lot in understanding this topic.</p></blockquote><p>This is a brief (but not rigorous) proof of the <strong>Central Limit Theorem (CLT)</strong>.</p><p>The CLT is one of the most fundamental results in probability theory, showing that the sum of many independent random variables tends toward a normal distribution, regardless of their original distribution.</p><p>Intuitively, the CLT explains why many natural phenomena follow the bell curve: individual randomness averages out into a predictable pattern.</p><p>For application of CLT and some advanced understanding, I will add this part if available.</p><hr><h1 id=the-clt-statement>The CLT Statement<a hidden class=anchor aria-hidden=true href=#the-clt-statement>#</a></h1><p>Let $$X_1, X_2, \dots, X_n$$ be i.i.d. (independent and identically distributed) random variables with $\mathbb{E}(X_j) = \mu$$ and $$\mathrm{Var}(X_j) = \sigma^2 &lt; \infty$.</p><p>Define:</p><p>$$
Z_n = \frac{\sum_{j=1}^n X_j - n\mu}{\sqrt{n} \cdot \sigma}
$$</p><p>Then for all $\tau \in \mathbb{R}$,</p><p>$$
\lim_{n \to \infty} \mathbb{P}(Z_n \leq \tau) = \Phi(\tau)
$$</p><p>Or equivalently,</p><p>$$
Z_n \xrightarrow{D} \mathcal{N}(0, 1)
$$</p><h1 id=proof-for-central-limit-theorem>Proof For Central Limit Theorem<a hidden class=anchor aria-hidden=true href=#proof-for-central-limit-theorem>#</a></h1><h2 id=notes>Notes<a hidden class=anchor aria-hidden=true href=#notes>#</a></h2><ul><li>This proof is sometimes called <strong>Lindeberg–Lévy CLT</strong>, requiring i.i.d. variables with finite variance.</li><li>A more general version (Lindeberg–Feller CLT) loosens these assumptions.</li></ul><hr><h2 id=1-the-road-map>1. The Road Map<a hidden class=anchor aria-hidden=true href=#1-the-road-map>#</a></h2><p>To ensure both clarity and completeness, let’s first outline the proof strategy. We will use the characteristic function method to prove the CLT. The ideas are simple:</p><blockquote><ol><li>There&rsquo;s a one-to-one mapping relationship between characteristic function and pdf/distribution.</li><li>If the characteristic functions of $$Z_n$$ and $\mathcal{N}(0,1)$ are equal for all $t$, then their distributions are identical, which completes the proof.</li></ol></blockquote><p>For the <strong>one-to-one mapping relationship</strong>, we should turn to <strong>Fourier Transform</strong>. I will add this part if available in the future. In this blog, we assume that the previous relationship holds. For the definition and some properties of <strong>Characteristic Function</strong>, see <a href=#appendix-characteristic-functions>Appendix</a> for reference.</p><hr><h2 id=2-characteristic-function-of-mathcaln0-1>2. Characteristic Function of $\mathcal{N}(0, 1)$<a hidden class=anchor aria-hidden=true href=#2-characteristic-function-of-mathcaln0-1>#</a></h2><p>The definition of <strong>Characteristic Function</strong> for $\text{ random variable } X ,$ is defined as follows:</p><p>$$
\varphi_X(t) = \mathbb{E}[e^{itX}]
$$</p><p>For standard Gaussian $\text{ random variable } N \sim \mathcal{N}(0, 1)$, we can obtain the <strong>Characteristic Function</strong> below:</p><p>$$
\begin{align*}
\varphi_N(t) &= \int_{- \infty}^{+ \infty} e^{itx} \cdot ; \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2} x^2} dx \[2em]
&= \frac{1}{\sqrt{2 \pi}} \int_{- \infty}^{+ \infty} e^{-\frac{1}{2}x^2 , + itx} dx \ \
&= \frac{1}{\sqrt{2 \pi}} \int_{- \infty}^{+ \infty} e^{- \frac{1}{2}(x - it)^2 + \frac{1}{2}(it)^2} dx \ \
&= e^{- \frac{1}{2}t^2} \left( \frac{1}{\sqrt{2 \pi}} \int_{- \infty}^{+\infty} e^{- \frac{1}{2}(x - it)^2}d(x-it) \right) \ \
&= e^{- \frac{1}{2} t^2}
\end{align*}
$$</p><p>To evaluate the integral, we complete the square in the exponent.</p><p>For $\frac{1}{\sqrt{2 \pi}} \int_{- \infty}^{+\infty} e^{- \frac{1}{2}(x - it)^2}d(x-it)$, the resulting integral is the total probability under the standard normal distribution (after a complex shift), and therefore equals 1.</p><hr><h2 id=3-characteristic-function-of-z_n>3. Characteristic Function of $$Z_n$$<a hidden class=anchor aria-hidden=true href=#3-characteristic-function-of-z_n>#</a></h2><h3 id=step-1-standardize-variables>Step 1: Standardize Variables<a hidden class=anchor aria-hidden=true href=#step-1-standardize-variables>#</a></h3><p>Let&rsquo;s define standardized variables:</p><p>$$
Y_j = \frac{X_j - \mu}{\sigma}, \quad \text{so that } \mathbb{E}[Y_j] = 0, \quad \mathrm{Var}(Y_j) = 1
$$</p><p>Then:</p><p>$$
Z_n = \frac{1}{\sqrt{n}} \sum_{j=1}^n Y_j
$$</p><p>This reformulates the normalized sum into a simpler form, preparing it for the characteristic function method.</p><h3 id=step-2-characteristic-function-of-y_j>Step 2: Characteristic Function of $$Y_j$$<a hidden class=anchor aria-hidden=true href=#step-2-characteristic-function-of-y_j>#</a></h3><p>The characteristic function of $Y_j$ is:</p><p>$$
\varphi_{Y_j}(t) = \mathbb{E}[e^{itY_j}]
$$</p><p>Using Taylor expansion:</p><p>$$
\begin{align*}
\varphi_{Y_j}(t) &= \mathbb{E}[\sum_{k=0}^{\infty} \frac{(itY_j)^k}{k !}]\ \
&= \sum_{k=0}^{\infty} \frac{(it)^k}{k!} \mathbb{E}[Y_j^k] \ \
&= 1 - \frac{1}{2} t^2 + o(t^2) \
\end{align*}
$$</p><h3 id=step-3-characteristic-function-of-z_n>Step 3: Characteristic Function of $$Z_n$$<a hidden class=anchor aria-hidden=true href=#step-3-characteristic-function-of-z_n>#</a></h3><p>Here are two properties of <strong>Characteristic Function</strong>:</p><blockquote><ol><li>Linearity: $\text{ Linearity}: ;\varphi_{aX + b}(t) = e^{ibt} \cdot \varphi_X(at)$</li><li>Independence: $\text{If random variable } X \text{ and } Y \text{ are independent }, ; \varphi_{X + Y}(t) = \varphi_X(t) \cdot \varphi_Y(t)$</li></ol></blockquote><p>The proof for this two properites can be seen in the <a href=#appendix-characteristic-functions>Appendix</a>.</p><p>$$
\begin{align*}
\varphi_{Z_n}(t) &= \varphi_{\frac{1}{\sqrt{n}} \sum_j Y_j}(t) \ \
&= \left( \varphi_{Y_j} \left( \frac{t}{\sqrt{n}} \right) \right)^n \ \
&= \left(1 - \frac{t^2}{2n} + o\left( \frac{t^2}{n} \right) \right)^n \
\end{align*}
$$</p><p>As $n \to \infty$, by the exponential limit identity:</p><p>$$
\lim_{n \to \infty} \varphi_{Z_n}(t) = \lim_{n \to \infty} \left(1 - \frac{t^2}{2n} + o\left(\frac{1}{n}\right)\right)^n = e^{- \frac{1}{2} t^2}
$$</p><h2 id=4-final-step>4. Final Step<a hidden class=anchor aria-hidden=true href=#4-final-step>#</a></h2><p>$$
\lim_{n \to \infty} \varphi_{Z_n}(t) = \varphi_{N}(t) \quad \Rightarrow \quad Z_n \xrightarrow{D} \mathcal{N}(0, 1) \quad \blacksquare
$$</p><p>This completes the proof of the Central Limit Theorem under the assumptions of i.i.d and finite variance.</p><hr><h1 id=appendix-characteristic-functions>Appendix: Characteristic Functions<a hidden class=anchor aria-hidden=true href=#appendix-characteristic-functions>#</a></h1><p>We only use 2 properities of Characteristic Functions. For more detailed description, please check [this article] (I will write this if available).</p><blockquote><ol><li>Linearity: $\varphi_{aX + b}(t) = e^{ibt} \cdot \varphi_X(at)$</li><li>Independence: $\text{If random variable } X \text{ and } Y \text{ are independent }, ; \varphi_{X + Y}(t) = \varphi_X(t) \cdot \varphi_Y(t)$</li></ol></blockquote><p>$\text{Proof for Linearity}$:</p><p>$$
\varphi_{aX+b}(t) = \mathbb{E}[e^{it(aX+b)}] = e^{itb} \mathbb{E}[e^{itaX}] =e^{itb} \varphi_X(at)
$$</p><p>$$\text{Proof for Independence}$$:</p><p>This property is a consequence of the factorization of the joint distribution for independent random variables.</p><p>$$
\text{Suppose } \alpha(X) \text{ and } \beta(Y) \text{ are two functions of X and Y.}
$$</p><p>$$
\begin{align*}
\mathbb{E}[\alpha(X)\beta(Y)] &= \iint_{D_{X, Y}} f_{X, Y}(x, y) \cdot \alpha(x) \beta(y) ;dx , dy \
&= \iint_{D_{X, Y}} f_X(x) , f_Y(y) \cdot \alpha(x) \beta(y) ;dx , dy \
&= \int_{D_X} f_X(x) , \alpha(x) , dx ; \cdot ; \int_{D_Y} f_Y(y), \beta(y) , dy \
&= \mathbb{E}[\alpha(X)] , \cdot , \mathbb{E}[\beta(Y)]
\end{align*}
$$</p><p>$$
\begin{aligned}
\varphi_{X + Y}(t) &= \mathbb{E}[e^{it(X + Y)}] \ \
&= \mathbb{E}[e^{itX} e^{itY}] \ \
&= \mathbb{E}[e^{itX}] , \mathbb{E}[e^{itY}] \ \
&= \varphi_X(t) , \varphi_Y(t)
\end{aligned}
$$</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://hanbinzheng.github.io/tags/probability/>Probability</a></li><li><a href=https://hanbinzheng.github.io/tags/math/>Math</a></li><li><a href=https://hanbinzheng.github.io/tags/clt/>CLT</a></li><li><a href=https://hanbinzheng.github.io/tags/gaussian/>Gaussian</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://hanbinzheng.github.io/>Hanbin Zheng</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>
---
title: Gradient, Directional Derivative, and Tangent Geometry — A Complete Guide
date: 2025-06-16 +80000
categories: [Math, Calculus]
author: Chat GPT mainly
tags: [Math, Vector Calculus]
---

> This post is generated by Chat GPT. If there's any mistake, please point it out.

This article develops a rigorous and coherent understanding of the **directional derivative**, **gradient**, and their geometric and analytic interpretations. Starting from first principles, we build up the theory from the definitions of **partial derivatives** and **directional derivatives**, through the notion of differentiability, to the consequences: the **existence and meaning of the gradient**, the **direction of steepest ascent**, and the **orthogonality between the gradient and level sets**.

---

## 1. Directional and Partial Derivatives: First Definitions

Let $$ f: \mathbb{R}^n \to \mathbb{R} $$ be a scalar-valued function, and $$ \mathbf{x}_0 \in \mathbb{R}^n $$ a point.

The **directional derivative** of $$ f $$ at point $$ \mathbf{x}_0 $$ in the direction $$ \mathbf{v} \in \mathbb{R}^n $$ is defined as:

$$
D_{\mathbf{v}} f(\mathbf{x}_0) := \lim_{\epsilon \to 0} \frac{f(\mathbf{x}_0 + \epsilon \mathbf{v}) - f(\mathbf{x}_0)}{\epsilon}
$$

- If $$ \|\mathbf{v}\| = 1 $$, this is interpreted as the rate of change of $$ f $$ in the **direction** of $$ \mathbf{v} $$.
- If $$ \|\mathbf{v}\| \neq 1 $$, the limit still exists, but it reflects both direction and **scaling**, hence loses interpretability as a *pure* "rate of change".

The **partial derivative** with respect to the $$ i $$-th coordinate is a special case where $$ \mathbf{v} = \mathbf{e}_i $$, the standard basis vector:

$$
\frac{\partial f}{\partial x_i}(\mathbf{x}_0) := D_{\mathbf{e}_i} f(\mathbf{x}_0)
$$

---

## 2. Differentiability and First-Order Approximation

A function $$ f: \mathbb{R}^n \to \mathbb{R} $$ is said to be **differentiable at** $$ \mathbf{x}_0 $$ if there exists a vector $$ \mathbf{a} \in \mathbb{R}^n $$ such that:

$$
f(\mathbf{x}_0 + \mathbf{h}) = f(\mathbf{x}_0) + \mathbf{a} \cdot \mathbf{h} + o(\|\mathbf{h}\|) \quad \text{as } \mathbf{h} \to 0
$$

- This $$ \mathbf{a} $$ is called the **differential vector** at $$ \mathbf{x}_0 $$.
- The existence of such an $$ \mathbf{a} $$ implies all partial derivatives exist and are continuous in a neighborhood of $$ \mathbf{x}_0 $$.

By plugging $$ \mathbf{h} = \epsilon \mathbf{e}_i $$ into the above definition:

$$
\frac{f(\mathbf{x}_0 + \epsilon \mathbf{e}_i) - f(\mathbf{x}_0)}{\epsilon} = \mathbf{a} \cdot \mathbf{e}_i + o(1) = a_i + o(1)
$$

Taking the limit as $$ \epsilon \to 0 $$, we recover:

$$
a_i = \frac{\partial f}{\partial x_i}(\mathbf{x}_0)
$$

Since $$ \mathbf{a} $$ is unique, we define:

$$
\nabla f(\mathbf{x}_0) := \mathbf{a} = \begin{bmatrix}
\frac{\partial f}{\partial x_1}(\mathbf{x}_0) \\
\vdots \\
\frac{\partial f}{\partial x_n}(\mathbf{x}_0)
\end{bmatrix}
$$

Thus, the gradient vector contains all first-order directional information of the function at $$ \mathbf{x}_0 $$.

---

## 3. Directional Derivative via the Gradient

Let $$ \mathbf{v} \in \mathbb{R}^n $$, not necessarily unit length. Using differentiability:

$$
f(\mathbf{x}_0 + \epsilon \mathbf{v}) = f(\mathbf{x}_0) + \nabla f(\mathbf{x}_0) \cdot (\epsilon \mathbf{v}) + o(\epsilon)
$$

Hence:

$$
D_{\mathbf{v}} f(\mathbf{x}_0) = \lim_{\epsilon \to 0} \frac{f(\mathbf{x}_0 + \epsilon \mathbf{v}) - f(\mathbf{x}_0)}{\epsilon} = \nabla f(\mathbf{x}_0)^T \cdot \mathbf{v}
$$

> ✅ **Conclusion**: The directional derivative in any direction $$ \mathbf{v} $$ is the dot product of the gradient with $$ \mathbf{v} $$.

---

## 4. Steepest Ascent and Descent

Let $$ \|\mathbf{v}\| = 1 $$. Since the directional derivative is:

$$
D_{\mathbf{v}} f(\mathbf{x}_0) = \|\nabla f(\mathbf{x}_0)\| \cdot \|\mathbf{v}\| \cdot \cos\theta
$$

- This is **maximized** when $$ \mathbf{v} $$ is in the same direction as $$ \nabla f(\mathbf{x}_0) $$
- **Minimized** (i.e., greatest decrease) when $$ \mathbf{v} $$ is in the opposite direction

> ✅ **Conclusion**: The gradient points in the direction of **steepest ascent**; the negative gradient points in the direction of **steepest descent**.

---

## 5. Why the Gradient is Orthogonal to Level Sets

Let $$ f: \mathbb{R}^n \to \mathbb{R} $$, and define the **level set** (or level surface):

$$
S := \{ \mathbf{x} \in \mathbb{R}^n \mid f(\mathbf{x}) = c \}
$$

Let $$ \mathbf{x}(t) $$ be a smooth curve lying on this surface such that $$ \mathbf{x}(t) = \mathbf{x}_t \in S $$. Then:

$$
f(\mathbf{x}(t)) \equiv c \Rightarrow \frac{d}{dt} f(\mathbf{x}(t)) = 0
$$

Applying the **vector chain rule** (multivariable chain rule):

$$
\frac{d}{dt} f(\mathbf{x}(t)) = \nabla f(\mathbf{x}(t))^T \cdot \mathbf{x}'(t) \Rightarrow \nabla f(\mathbf{x}_t)^T \cdot \mathbf{x}'(t) = 0
$$

So for any **tangent vector** $$ \mathbf{x}'(0) $$ to the level set at $$ \mathbf{x}_0 $$, it holds that:

$$
\nabla f(\mathbf{x}_t) \perp \mathbf{x}'(t)
$$

> ✅ **Conclusion**: The gradient is **orthogonal** to the level surface $$ S $$ at $$ \mathbf{x}_t $$.

---

## 6. From Gradient to Jacobian

For scalar-valued functions $$ f: \mathbb{R}^n \to \mathbb{R} $$, the gradient is the transpose of the Jacobian:

$$
\nabla f(\mathbf{x}) = J_f(\mathbf{x})^T \in \mathbb{R}^n
$$

For vector-valued functions $$ \mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m $$, the **Jacobian matrix** is:

$$
J_{\mathbf{f}}(\mathbf{x}) = \begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n}
\end{bmatrix} \in \mathbb{R}^{m \times n}
$$

In this generalization:

- Gradient $$ = $$ Jacobian of scalar function (as a row or column vector)
- Chain rule becomes: $$ J_{\mathbf{f} \circ \mathbf{g}}(t) = J_{\mathbf{f}}(\mathbf{g}(t)) \cdot J_{\mathbf{g}}(t) $$

---

## 7. Summary Table

| Concept | Notation | Definition | Dimension |
|--------|----------|------------|-----------|
| Partial Derivative | $$ \frac{\partial f}{\partial x_i} $$ | Rate of change along axis $$ x_i $$ | Scalar |
| Directional Derivative | $$ D_{\mathbf{v}} f(\mathbf{x}_0) $$ | $$ \nabla f(\mathbf{x}_0)^T \cdot \mathbf{v} $$ | Scalar |
| Gradient | $$ \nabla f(\mathbf{x}_0) $$ | First-order approximation vector | $$ \mathbb{R}^n $$ |
| Jacobian | $$ J_{\mathbf{f}} $$ | Matrix of all partial derivatives | $$ \mathbb{R}^{m \times n} $$ |

---

## Appendix: Vector Chain Rule

Let $$ \mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m $$, and $$ \mathbf{x}(t): \mathbb{R} \to \mathbb{R}^n $$. Then:

$$
\frac{d}{dt} \mathbf{f}(\mathbf{x}(t)) = J_{\mathbf{f}}(\mathbf{x}(t)) \cdot \mathbf{x}'(t)
$$

This is the generalization of the chain rule to vector-valued functions, and underpins the geometric proof of gradient orthogonality.
